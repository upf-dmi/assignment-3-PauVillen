
---
title: "Hands_on_III"
author: "Pau Villén Vidal (pau.villenvidal01@estudiant.upf.edu)"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: show
    df_print: paged
fontsize: 12pt
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


# Exercise 1 - Build a Random Forest model to classify COVID-19 severity

## Introduction

Proteomic profiling provides a powerful approach to characterize systemic biological responses to infection. By measuring circulating protein levels, it is possible to identify molecular signatures associated with disease severity. Machine learning methods are particularly well suited for analyzing high-dimensional omics data, as they can capture complex, non-linear relationships between predictors and clinical outcomes.
In this analysis, we aim to develop a classification model to distinguish between severe and non-severe COVID-19 patients using proteomic data. A Random Forest algorithm is implemented due to its robustness, ability to handle high-dimensional data, and built-in feature importance estimation.
The workflow includes data preprocessing, missing value handling, feature filtering, model training, hyperparameter tuning, and performance evaluation using cross-validation and an internal test set. Model performance is assessed using metrics such as accuracy, sensitivity, specificity, and the area under the ROC curve (AUC).
This approach allows us to evaluate the predictive potential of proteomic biomarkers and assess the reliability of machine learning models in a clinical classification setting.

## Data Preparation

### Loading required packages

```{r}
# install.packages("readxl")
# install.packages("tidyverse")
# install.packages("janitor")
# install.packages("caTools")
# install.packages("randomForest")
# install.packages("tidymodels")
# install.packages("parsnip")
# install.packages("caret")
# install.packages("recipies")
# install.packages("pROC")
```

### Importing proteomic data and libraries

```{r}

library(tidyverse)
library(readxl)
library(janitor) #clean_names function
library(dplyr)
library(caret)
library(recipes)
library(pROC)
library(rsample)
library(randomForest)

df <- read_excel("table_s3.xlsx", sheet = 2)
df_cleaned <- clean_names(df) 

df_metdata <- read_excel("table_s1.xlsx", sheet = 2)
df_metdata_cleaned <- clean_names(df_metdata) 

```

### Data reshaping and cleaning

Now we have patients in columns and proteins in rows. To run most analysis, the structure needs to be flipped. 

- Columns (variable) --> something we measure (proteins) 

- Rows (observations) --> individual entity (patients)

```{r}
#Extract actual patients IDs from the first row
new_headers <- df_cleaned[1, ] %>%
  select(patient_id:last_col()) %>% #start at patient_id til the enc
  unlist() %>%
  as.character()

#Rename the columns of the dataframe uing these real patient IDs
colnames(df_cleaned)[3:ncol(df_cleaned)] <- new_headers
#Columns 1 and 2 stay as they are, only columns 3 to onwards become XG1, XG2, etc

#Now transpose the table!
df_cleaned_final <- df_cleaned %>%
  slice(-1) %>%                         # Remove that first row now that we've used it for names
  pivot_longer(
    cols = all_of(new_headers),         # Pivot all the patient columns (XG1, XG2...)
    names_to = "patient_id",            # New column to hold the patient names
    values_to = "expression"            # New column to hold the numeric values
  ) %>%
  select(-gene_symbol) %>%
  pivot_wider(
    names_from = proteins_metabolites,          
    values_from = expression
  ) %>%
  # Convert the protein columns to numeric for the random forest
  mutate(across(-patient_id, as.numeric))


colnames(df_metdata_cleaned)[1] <- "patient_id" #changed the name from patient_id_a to patient_id to match the column names between the 2 dataframes

#Added in the df_final the group_d (severity) from df_metdata_cleaned
df_metdata_cleaned <- df_metdata_cleaned %>% select(patient_id, group_d)

```

### Merging clinical metadata

```{r}
df_final <- left_join(df_cleaned_final, df_metdata_cleaned, by = "patient_id") #2:non-Severe; 3: Severe
#As this data contains also non-COVID and healthy patients, and the exercise tells to filter only between non-severe and severe, I will filter the df_final to only have non-severe and severe patients (only 2 and 3).

df_final <- df_final %>%
  filter(group_d %in% c(2, 3)) %>%
  mutate(group_d = factor(group_d, levels = c(2,3), labels = c("non_Severe", "Severe"))) %>%
  select(-patient_id) #remove ID so it's not used as a predictor
table(df_final$group_d) #to see how many non-severe and severe patients are there and if we have removed the other groups correctly
#Now df_final contains all proteins in columns, patients in rows and in the last column it appears group_d (and only contians non-severe and severe patients)
```

## Missing data handling

### Exploratory NA analysis

```{r}
#Now let's see the NA values (because RF cannot work if there are NA values)
sum(is.na(df_final)) #Total NA values
```

### Removing proteins with >20% missing values

Remove proteins with more than 20% missing values (this removes unreliable features). Because if a protein is missing in 80% of the patients, and I fill those gaps with the median of the 20% who actually had it, I am essentially inventing 80% of the data. And therefore, the RF might find a pattern in that protein, but that protein isn't biological, it's just a result of my imputation method!!!

```{r}
na_threshold <- 0.2
cols_to_keep <- colMeans(is.na(df_final)) <= na_threshold
df_final_0.2_removed <- df_final[ , cols_to_keep]
```

At this point, we have removed some proteins that contained more than 20% missing values.

### Median imputation

```{r}
df_final_cleaned <- df_final_0.2_removed %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))
sum(is.na(df_final_cleaned)) #now we have imputed the remaining NAs with the median
```

### Final feature cleaning

```{r}
#clean the columns in the df_final_cleaned becasue RF algorithm got crazy
df_final_cleaned <- df_final_cleaned %>% 
  clean_names()
```


## Initial random forest model (manual approach)

At this point, I have obtained a tidy data format necessary for ML. As we have ~1600 proteins (features) but very few patients, Random Forest model is a great choice because it handles "wide" data well. 

```{r}

rf_model <- randomForest(as.factor(group_d) ~ .,
                         data = df_final_cleaned,
                         importance = TRUE, #essential for exercise 2, where need to identify the top 25 proteins. so it tells R to calculate which proteins were most helpful in splitting the severe from non-severe groups
                         ntree = 500)

print(rf_model)

```

## Structured machine learning workflow (Recipe-Based Approach)

Recipe packages makes the workflow more professional, reproducible, and robust. Defines the data preprocessing steps as a "blueprint" and in exercise 3 when doing the test, I won't have to manually repeat the cleaning, just apply this blueprint. 

### Cohort splitting (80/20 Stratified Split)

```{r}


# Prepare data
# Ensure names are clean and variables are ready
df_final_preclean <- df_final %>% janitor::clean_names()

#--- COHORT SPLIT & INTERNAL DIVISION ------
set.seed(123)

#Create an 80/20 split stratified by the outcome (group_d)
data_split <- initial_split(df_final_preclean, prop = 0.8, strata = group_d)

#Development cohort (80%)
training_data <- training(data_split) #will have 24 observation, which is a 0.8% of the 31 ones initially

#Internal testing (20%)
internal_test_data <- testing(data_split)

```

### Cross-Validation setup (10-Fold CV)

```{r}

#Define the recipe (blueprint) (THIS IS DONE IN THE TEACHER SLIDES!!!!)
rf_recipe <- recipe(group_d ~ ., data = training_data) %>%
  step_nzv(all_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors())

# set up training controls
# This handles the 10-fold CV and collects data for the ROC curve
ctrl <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",       # Necessary to plot the ROC later
  classProbs = TRUE,               # Necessary for AUC calculation
)

```

### Hyperparameter tuning (mtry Grid)

```{r}
# Define tuning grid
# Testing different numbers of predictors at each split
p <- ncol(df_final_preclean) - 1 
mtry_grid <- data.frame(mtry = c(1, 2, floor(sqrt(p)), floor(p/3), 10, 30))

```

### Model training with caret

```{r}

rf_model_2 <- train(
  rf_recipe, 
  data = training_data, 
  method = "rf",
  trControl = ctrl,
  tuneGrid = mtry_grid,
  metric = "Accuracy",                 
  ntree = 500,
  importance = TRUE
)
```


## Model evaluation

### Cross-Validation performance

```{r}
#------ RESULTS ANALYSIS------

# Performance during Cross-validation (the 80%)
print(rf_model_2)
```

### Internal hold-out test performance

```{r}
# Performance on the Internal Hold-out (the 20%)
## This proves the model works on "unseen" data before going to the external cohort
internal_preds <- predict(rf_model_2, newdata = internal_test_data)
internal_cm <- confusionMatrix(internal_preds, internal_test_data$group_d)

print("----Internal 20% test results:-----")
print(internal_cm)

```

### ROC curve and AUC

```{r}
# Plot the Realistic ROC Curve
# Using the cross-validated predictions (prevents the "perfect square" look)
roc_obj <- roc(rf_model_2$pred$obs, rf_model_2$pred$Severe)

# Draw the plot (this creates 'plot.new' inside the file)
{
  plot(roc_obj, col = "blue", lwd = 3, main = "ROC Curve (10-Fold Cross-Validation)")

#Add the lines (these will now work)
abline(a = 0, b = 1, lty = 2, col = "grey")

# Add the legend
legend("bottomright", 
       legend = paste("AUC =", round(auc(roc_obj), 3)), 
       col = "blue", 
       lwd = 3, 
       bty = "n")

}
```


## Interpretation and discussion

As shown in the updated results, the "manual" model (Random Forest with 500 trees) achieved an Out-of-Bag (OOB) error rate of 19.35%. In contrast, the recipe-based model (using the 80/20 split and cross-validation) showed a slightly higher error rate on the internal test set of 28.57% (equivalent to 71.43% accuracy).

The difference in performance is largely due to how each approach handles the data:

- Manual Model Performance: This model utilized all 31 samples (18 non-Severe, 13 Severe) to calculate its error estimate. By manually filtering proteins with >20% NAs, it likely retained a more concentrated set of high-quality predictors.

- Recipe-based Model Performance: This model was restricted to a smaller training set (80% of the cohort) to maintain strict architectural separation. Because it had fewer samples to learn from, its accuracy on the internal 20% hold-out was naturally lower at 71.43%.

- Feature Handling: The recipe model utilizes ```step_nzv()```, which is a conservative filter that removes variables with near-zero variance. This may have kept "noisy" proteins that the manual model's stricter NA threshold successfully eliminated.

To further validate the model, the ROC Curve and the Confusion Matrix were analyzed. The model achieves an outstanding AUC (Area Under the Curve) of 0.907. This indicates a roughly 91% probability that the model will correctly rank a randomly chosen Severe patient higher (in terms of severity risk) than a non-Severe patient.

- Cross-Validation: During the 10-fold cross-validation tuning process, the model reached a peak Accuracy of 81.67% (with mtry=40).

- Internal Testing: On a hold-out test set (the 20% internal division), the model maintained an Accuracy of 71.43%.

(mtry is a tunning parameter that controls the randomness and diversity of the trees in the forest. To decide which protein best separates Severe from non-Severe), it doesn't look at all 1639 proteins at once, instead the model randomly picks a subset of proteins, and the size of that subset is mtry, so the tree can only choose the best protein from that specific subset. So mtry = 30 means that the model had enough options to fins strong patterns but enough randomness to stay flexible, reaching a peak accuracy of 0.8167).

On the "unseen" 20% internal division, the model achieved an overall accuracy of 71,43%, correctly classifying 3 non_Severe and 2 Severe patients, misidentifying 1 non_Severe patient as Severe and 1 Severe patient as non_Severe one. Using "non_Severe" as the positive class, the model showed a sensitivity of 75% and a specificity of 66,67%. 


## Conclusion

All in all, the proteomic signature is clearly a strong biological marker for disease severity. The model is highly sensitive, as shown by the sharp rise in the ROC curve. However, the drop from 81% accuracy in training to 71% in the internal test suggests some overfitting—likely due to the high number of predictors (1,638 proteins) compared to the small number of samples (24 patients in the training set). While the manual model shows a technically lower error rate (19.35%), the recipe-based model is the more robust choice for Exercise 3. It adheres to the Training Process Architecture by ensuring that the model's generalizability is tested on a truly "unseen" internal division before moving to the external cohort. The recipe model's AUC of 0.907 confirms that, despite the higher internal test error, it remains an excellent tool for separating non-Severe from Severe patients.


# Exercise 2 – Differential Expression / Biomarker Discovery Analysis

## Objective and overview

The goal of Exercise 2 is to identify and characterize proteins that are significantly associated with differences between predefined clinical groups. Through differential expression analysis and statistical testing, we aim to detect potential biomarkers that distinguish between conditions of interest and evaluate their biological and clinical relevance.

The analysis is based on a proteomic expression dataset containing quantitative measurements of circulating proteins across multiple patient samples. These measurements are integrated with corresponding clinical metadata, including group classification variables that define the comparison of interest (e.g., disease status, severity, or treatment group).
The study follows a comparative observational design in which protein expression levels are statistically compared between groups. Appropriate preprocessing, quality control and multiple testing correction procedures are applied to ensure robust and reproducible results.


## Data Loading and setup

### Required packages 

```{r}
# install.packages("tidyverse")
# install.packages("caret")
# install.packages("janitor")
# install.packages("dplyr")
# install.packages("ggplot2")
# install.packages(c("iml", "fastshap")) 
# install.packages("pheatmap")  # if not installed
library(pheatmap)
library(tidyverse)
library(caret)
library(janitor)
library(dplyr) #for calibration plot
library(ggplot2) #for calibration plot
library(iml) #for SHAP analysis
library(fastshap) #for SHAP analysis
```

## Top 25 proteins and importing data

```{r}
# extract importance
importance_results <- varImp(rf_model_2, scale = FALSE)

# create top 25 list
top_25_list <- importance_results$importance %>%
  as.data.frame() %>%
  rownames_to_column(var = "Protein_ID") %>%
  # Fix the case: Ensure Protein_ID is Uppercase to match the metadata
  mutate(Protein_ID = toupper(Protein_ID)) %>%
  arrange(desc(Severe)) %>% 
  slice_head(n = 25)

#prepare comparison data
# We load the metadata and ensure the mapping ID is also Uppercase
df_table_5_meta <- read_excel("table_s5.xlsx", sheet = 2) %>% 
  clean_names() %>%
  slice(-1) %>% # Remove the row used for patient IDs
  select(proteins_metabolites, gene_symbol) %>%
  distinct() %>%
  mutate(proteins_metabolites = toupper(proteins_metabolites))

# merge
# Note: 'Protein_ID' from your list matches 'proteins_metabolites' from the metadata
top_25_named <- top_25_list %>% 
  left_join(df_table_5_meta, by = c("Protein_ID" = "proteins_metabolites"))

```


## Results visualization

### Variable importance plot
```{r}
# view results
print(top_25_named)

# plot
# If the gene symbol exists, use it. If not, fallback to the Protein_ID.
top_25_named <- top_25_named %>%
  mutate(Label = ifelse(!is.na(gene_symbol) & gene_symbol != "", gene_symbol, Protein_ID))

ggplot(top_25_named, aes(x = reorder(Label, Severe), y = Severe)) +
  
  # Draw the lines 
  geom_segment(aes(xend = Label, yend = 0), color = "grey50", linewidth = 1) +
  
  # Draw the points at the end 
  geom_point(color = "steelblue", size = 2) +
  
  coord_flip() +
  theme_minimal() +
  
  labs(
    title = "Exercise 2: Top 25 Protein Predictors",
    x = NULL,              
    y = "Importance"       
  ) +
  
  theme(
    panel.grid = element_blank(), 
    
    # Draw border
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1),
    
    axis.ticks.x = element_line(color = "black"),
    axis.ticks.y = element_line(color = "black")
  )
```

### Volcano plot
```{r}
library(dplyr)
library(ggplot2)
library(ggrepel)

#Pick the top 25 proteins from your RF importance table 
top_proteins <- tolower(top_25_named$Protein_ID)
# Choose the dataset that contains protein values + group labels 
df <- df_final_cleaned 
# Compute log2FC (Severe vs non_Severe) + p-values for those proteins -
volcano_df <- lapply(top_proteins, function(p) {
  
  if (!p %in% colnames(df)) return(NULL)
  
  x_sev <- df %>% filter(group_d == "Severe") %>% pull(!!sym(p))
  x_non <- df %>% filter(group_d == "non_Severe") %>% pull(!!sym(p))
  
  x_sev <- x_sev[is.finite(x_sev)]
  x_non <- x_non[is.finite(x_non)]
  
  if (length(x_sev) < 2 || length(x_non) < 2) return(NULL)
  
  # Robust effect size: median log2 fold change
  log2fc <- log2(median(x_sev) + 1e-9) - log2(median(x_non) + 1e-9)
  
  # Robust test for omics distributions
  pval <- suppressWarnings(wilcox.test(x_sev, x_non)$p.value)
  
  data.frame(Protein_ID = p, log2FC = log2fc, p_value = pval)
}) %>%
  bind_rows() %>%
  mutate(
    p_adj = p.adjust(p_value, method = "BH"),
    negLog10FDR = -log10(p_adj),
    Significant = p_adj < 0.05 & abs(log2FC) >= 1
  )

# Add gene symbols from mapping table 
volcano_df <- volcano_df %>%
  left_join(
    top_25_named %>%
      mutate(Protein_ID = tolower(Protein_ID)) %>%
      select(Protein_ID, gene_symbol),
    by = "Protein_ID"
  ) %>%
  mutate(Label = ifelse(!is.na(gene_symbol) & gene_symbol != "", gene_symbol, Protein_ID))

ggplot(volcano_df, aes(x = log2FC, y = negLog10FDR)) +
  geom_point(aes(shape = Significant), size = 3) +
  geom_vline(xintercept = c(-1, 1), linetype = "dashed") +
  geom_hline(yintercept = -log10(0.05), linetype = "dashed") +
  ggrepel::geom_text_repel(
    data = volcano_df %>% arrange(p_adj) %>% slice(1:10),
    aes(label = Label),
    max.overlaps = 30
  ) +
  labs(
    title = "Volcano plot (Top 25 RF proteins): Severe vs non-Severe",
    x = "log2 Fold Change (Severe / non-Severe)",
    y = "-log10(FDR)"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank()
  )

```

The volcano plot of the Top 25 Random Forest predictors demonstrates that several highly important proteins, including SAA2 and CRP, show both large effect sizes and strong statistical significance. This concordance between machine learning importance and classical differential analysis supports the biological relevance of the identified proteomic signature. Most significant proteins are upregulated in Severe patients, consistent with an inflammatory response profile.

### Heatmap of significant features

```{r}
library(pheatmap)
library(dplyr)

# 1) Matrix (patients x proteins)
significant_proteins <- volcano_df %>%
  filter(p_adj < 0.05 & abs(log2FC) >= 1) %>%
  pull(Protein_ID)

heatmap_data <- training_data %>%
  select(all_of(significant_proteins), group_d)

expr_matrix <- heatmap_data %>%
  select(-group_d) %>%
  as.matrix()

# 2) Give the matrix valid rownames (must match annotation_row rownames)
rownames(expr_matrix) <- paste0("S", seq_len(nrow(expr_matrix)))

# 3) Scale by protein (column-wise) -> keep same rownames
expr_matrix_scaled <- scale(expr_matrix)

# 4) Annotation with matching rownames
matched_labels <- volcano_df$Label[match(colnames(expr_matrix_scaled), volcano_df$Protein_ID)]
colnames(expr_matrix_scaled) <- matched_labels
annotation_df <- data.frame(Group = heatmap_data$group_d)
rownames(annotation_df) <- rownames(expr_matrix_scaled)

# 5) Plot
pheatmap(
  expr_matrix_scaled,
  annotation_row = annotation_df,
  cluster_rows = TRUE,
  cluster_cols = TRUE,
  show_rownames = FALSE,
  main = "Heatmap of Significant Proteins"
)


```

The heatmap of statistically significant proteins demonstrates distinct expression patterns between Severe and non-Severe patients. The significant proteins show elevated expression in Severe individuals, consistent with the differential analysis and Random Forest feature importance results. Although only a limited number of proteins met the predefined significance threshold, their expression profiles reveal a coherent severity-associated signature.

## Biological interpretation

### Annotation / Feature mapping

```{r}
# Make sure Protein_ID in volcano_df matches case
volcano_df$Protein_ID <- tolower(volcano_df$Protein_ID)
top_25_named$Protein_ID <- tolower(top_25_named$Protein_ID)

feature_mapping <- top_25_named %>%
  left_join(
    volcano_df %>%
      select(Protein_ID, log2FC, p_adj),
    by = "Protein_ID"
  ) %>%
  arrange(desc(Severe))   # Severe column = RF importance score

print(feature_mapping)

```

Feature mapping of the top 25 Random Forest predictors revealed strong concordance between machine learning importance and statistical significance. Several highly ranked proteins, including SAA2, CRP, SERPINA3, HP and C6, demonstrated both large effect sizes and highly significant adjusted p-values. These proteins are well-known components of the acute-phase inflammatory response and complement cascade, which are strongly associated with severe COVID-19. The predominance of positively regulated inflammatory markers in severe patients suggests that the Random Forest model captures a biologically coherent systemic inflammatory signature. This agreement between predictive modeling and classical differential analysis supports the robustness and biological plausibility of the identified proteomic severity signature.

## Conclusions

The gene symbols appearing next to protein IDs (like SAA1, CRP, and HP) mean that the model successfully identified the same high-level biological markers that the researchers focused on. In the raw data, these are just technical accession numbers (like P02741), but by mapping them to gene symbols, it has been proven that the model "picked" the COVID-19 markers. If a protein has a gene symbol in the table, it is a known, named protein that the researchers also analyzed.

Based on the shared results obtained in this table, SAA1, SAA2 and CRP are the primary markers of the cytokine storm and systemic inflammation. SERPING1 and SERPINA3 indicate the activation of the body's innate defense and blood-clotting regulations. LUM and ITIH3 are linked to how the body repairs or scars connective tissue, particularly in the lungs. 


# Exercise 3 – External Validation of the Random Forest Model

## Objective
- Evaluate the generalizability of the trained Random Forest model  
- Assess performance on an independent external cohort  

## Independent test cohort preparation

### Importing the external dataset
- Loading supplementary table data  

```{r}
df_test_raw <- read_excel("table_s4.xlsx", sheet = 2) %>%
  clean_names()
```

- Cleaning column names and clinical label definition

```{r}
#Comparison based on supplementary table 4
df_metdata_cleaned_2 <- df_metdata_cleaned %>%
  mutate(
    group = case_when(
      group_d %in% c(0, 1) ~ "Non-COVID-19",
      group_d == 2 ~ "Non-severe",
      group_d == 3 ~ "Severe",
      TRUE ~ NA_character_
    ),
    group = factor(group, levels = c("Non-severe", "Severe"))
  ) %>%
  select(patient_id, group) %>%
  filter(group %in% c("Non-severe", "Severe"))
```

### Data reshaping and formatting

- Header reconstruction  
```{r}
new_headers <- df_test_raw[1, ] %>%
  select(patient_id:last_col()) %>% 
  unlist() %>%
  as.character()
colnames(df_test_raw)[3:ncol(df_test_raw)] <- new_headers
```

- Pivoting to tidy format  and convert expression values to numeric

```{r}
#pivot to tidy format
df_test_raw_final <- df_test_raw %>%
  slice(-1) %>%                     
  pivot_longer(
    cols = all_of(new_headers),         
    names_to = "patient_id",         
    values_to = "expression"           
  ) %>%
  select(-gene_symbol) %>%
  pivot_wider(
    names_from = proteins_metabolites,          
    values_from = expression
  ) %>%
  mutate(across(-patient_id, as.numeric))

df_test_with_labels <- df_test_raw_final %>%
  inner_join(df_metdata_cleaned_2, by = "patient_id") %>%
  mutate(group = recode(group, "Non-severe" = "non_Severe")) %>%
  mutate(group = factor(group, levels = c("non_Severe", "Severe"))) %>%
  clean_names()
```


## Feature alignment with training model

### Extracting model predictors
- Retrieving selected protein features from the trained model  

```{r}
model_predictors <- rf_model_2$finalModel$xNames
# model_predictors
```

### Handling missing features
- Identifying absent predictors  
- Adding missing columns (NA padding)  
- Ensuring identical feature structure  

```{r}
missing_features <- setdiff(model_predictors, colnames(df_test_with_labels))
# missing_features

df_test_with_labels[missing_features] <- NA #Add the missing columns to the test set, filled with NAs
#This is done because RF requires the exact same column names in the exact order as the training data.

x_test <- df_test_with_labels %>%
  dplyr::select(all_of(model_predictors))
```

## Model application to independent cohort

### Generating class predictions and predicted probabilities

```{r}
#Predict disease severity
#use the rf_model_2 to see if it can correctly guess which proteins are severe
# predictions (make sure these are vectors, not data frames) --> Apply trained model to the test cohort
test_predictions <- predict(rf_model_2, newdata = x_test)
test_probabilities <- predict(rf_model_2, newdata = x_test, type = "prob")

# If predict() returned a data frame (tidymodels), extract the column:
if (is.data.frame(test_predictions)) test_predictions <- test_predictions$.pred

# Probabilities column name can differ: "Severe" (caret) vs ".pred_Severe" (tidymodels)
prob_severe <- if (is.data.frame(test_probabilities)) {
  if ("Severe" %in% colnames(test_probabilities)) test_probabilities$Severe else test_probabilities$.pred_Severe
} else {
  test_probabilities[, "Severe"]
}
```

### Building the results table  

```{r}
# Results table
test_results_table <- tibble(
  patient_id = df_test_with_labels$patient_id,
  Actual = df_test_with_labels$group,
  Predicted_Class = test_predictions,
  Prob_Severe = prob_severe
) %>%
  mutate(Correct = ifelse(Actual == Predicted_Class, "Correct", "Wrong"))

print(test_results_table)
```

### Calibration plot and Brierer score

Recommended when n = 10. When converting the severe and non-severe cases, the ones that where wrongly assigned, patients XG46 and XG22, which were severe when actually they were non-severe, they have been correctly numerically assigned. In other words, instead of putting them a 1 (severe), they have a 0 for non-severe. 

```{r}
# Convert actual class to numeric (1 = Severe, 0 = non_Severe)
calibration_df <- test_results_table %>%
  mutate(
    Actual_numeric = ifelse(Actual == "Severe", 1, 0)
  )

# Fit logistic regression of observed outcome vs predicted probability
calibration_model <- glm(Actual_numeric ~ Prob_Severe,
                         data = calibration_df,
                         family = binomial)

# Create smooth calibration curve
prob_seq <- seq(0, 1, length.out = 100)

calibration_curve <- data.frame(
  Prob_Severe = prob_seq,
  Observed = predict(calibration_model,
                     newdata = data.frame(Prob_Severe = prob_seq),
                     type = "response")
)

# Plot
ggplot(calibration_df, aes(x = Prob_Severe, y = Actual_numeric)) +
  geom_point(size = 3) +
  geom_line(data = calibration_curve,
            aes(x = Prob_Severe, y = Observed),
            color = "blue", linewidth = 1) +
  geom_abline(slope = 1, intercept = 0,
              linetype = "dashed",
              color = "red") +
  labs(
    title = "Calibration Plot – External Cohort",
    x = "Predicted Probability (Severe)",
    y = "Observed Proportion (Severe)"
  ) +
  theme_minimal() + 
  theme(
    panel.grid = element_blank(),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
  )

brier_score <- mean((calibration_df$Prob_Severe - calibration_df$Actual_numeric)^2)
brier_score
```
The red dashed line represent the perfect calibration, while the blue one is the model's calibration. The blue points represent each patient.
As for the Brier score, closer to 0 means a perfect prediction, and closer to 0.25 means ranodm guessing. 

Calibration performance was assessed in the external cohort using a calibration plot and the Brier score. The predicted probabilities showed strong agreement with observed outcomes, with clear separation between non-Severe and Severe patients. The Brier score was 0.139, indicating good probability accuracy and substantially outperforming random classification (Brier ≈ 0.25 for balanced binary outcomes). Although the calibration curve demonstrates strong alignment between predicted and observed outcomes, the small sample size (n = 10) still limits the precision of these estimates and warrants cautious interpretation.

## Performance evaluation on external cohort

### Confusion matrix, accuracy, sensitivity, specificity and Cohen's kappa

```{r}
# Confusion matrix (predicted vs truth)
final_metrics <- confusionMatrix(
  factor(test_results_table$Predicted_Class, levels = levels(test_results_table$Actual)),
  test_results_table$Actual,
  positive = "Severe"
)

print(final_metrics)
```



## Discussion

### Generalizability of the proteomic signature  

The external validation results indicate that the proteomic signature identified during model training maintains strong discriminatory performance in an independent patient cohort. Achieving 90% accuracy and correctly identifying all Severe cases suggests that the selected protein predictors capture biologically meaningful patterns associated with disease severity. Importantly, the model was applied to fully independent data that underwent preprocessing using parameters derived exclusively from the training cohort, reducing the risk of information leakage. This strengthens confidence that the observed performance reflects true generalizability rather than overfitting to the original dataset.

### Impact of small sample size (n = 10)  

Despite the strong performance metrics, the small size of the external cohort (n = 10) limits the statistical certainty of the results. With such a small sample, a single misclassification substantially affects accuracy, specificity, and other derived metrics. Consequently, confidence intervals around these estimates are likely wide. The perfect sensitivity observed (1.00) is encouraging but should be interpreted cautiously, as performance estimates in small samples can be unstable. Therefore, while the findings are promising, they cannot be considered definitive evidence of robustness.

### Clinical and biological implications  

The ability to correctly identify Severe patients is particularly relevant in a clinical context, where early recognition of high-risk individuals is critical for decision-making and resource allocation. The fact that the model achieved complete sensitivity suggests potential value in triaging patients based on proteomic profiling. Furthermore, the consistency of top predictive proteins across training and external validation cohorts supports their biological relevance in the pathophysiology of disease severity. These proteins may represent candidates for further mechanistic investigation or biomarker development.

### Need for larger-scale validation  

Although the external validation results are encouraging, validation in larger and more diverse cohorts is essential before clinical application can be considered. Larger studies would allow for more precise estimation of performance metrics, assessment of calibration, and evaluation across different demographic or clinical subgroups. Additionally, prospective validation in real-world clinical settings would be necessary to confirm reproducibility and practical utility. Future work should therefore focus on expanding cohort size and testing the model in independent populations to establish its reliability and clinical applicability.

## Conclusion

The final Random Forest model trained on the full training cohort demonstrated strong performance on the independent test cohort (n=10). Because it was used the ```recipies``` framework, the test data was automatically preprocessed (imputed and normalized) using the parameters established during the training phase. The model achieved an accuracy of 90%, correctly classifying 9 out of 10 patients. Importantly, all Severe patients were correctly identified (Sensitivity = 1.00), while one non-Severe patient was misclassified (Specificity = 0.83). The balanced accuracy was 0.92 and Cohen’s Kappa was 0.80, indicating strong agreement between the model's predictions and the actual clinical labels. These results suggest that the proteomic signature generalizes well to independent patients and maintains strong discriminatory ability. The model's perfromance on the independent cohort (90% accuracy) is notably higher than its performance on the internal 20% hold-out (71,43%). This suggests that the proteomic signature is highly robust biological marker for severity that remains consistent across different patient groups. Although performance is strong, the small size of the independent test cohort results in wide confidence intervals and warrants validation in larger cohorts.


# session info {.unnumbered}

```{r, results='asis',  echo=FALSE, message=FALSE }
sessionInfo()
```
